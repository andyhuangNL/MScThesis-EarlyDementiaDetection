{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_model1 = pd.read_csv('../data/predictions_model_visit1.csv')\n",
    "predictions_model2 = pd.read_csv('../data/predictions_model_visit12.csv')\n",
    "predictions_model3 = pd.read_csv('../data/predictions_model_visit123.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge predictions from all three models on ID\n",
    "ensemble_df = predictions_model1.merge(predictions_model2, on='ID', suffixes=('_model1', '_model2')) \\\n",
    "                                .merge(predictions_model3, on='ID')\n",
    "\n",
    "# Rename columns for clarity \n",
    "ensemble_df.rename(columns={\n",
    "    'Predicted_Label_model1': 'Pred_Label_Model1',\n",
    "    'Predicted_Label_model2': 'Pred_Label_Model2',\n",
    "    'Predicted_Label': 'Pred_Label_Model3',\n",
    "    'True_Label': 'True_Label'\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform majority voting\n",
    "ensemble_df['Ensemble_Prediction'] = np.apply_along_axis(\n",
    "    lambda x: np.bincount(x).argmax(), axis=1,\n",
    "    arr=ensemble_df[['Pred_Label_Model1', 'Pred_Label_Model2', 'Pred_Label_Model3']].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the necessary columns\n",
    "ensemble_df_cleaned = ensemble_df[['ID', 'Pred_Label_Model1', 'Pred_Label_Model2', 'Pred_Label_Model3', 'Ensemble_Prediction', 'True_Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Ensemble:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        31\n",
      "           1       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.92        48\n",
      "   macro avg       0.91      0.91      0.91        48\n",
      "weighted avg       0.92      0.92      0.92        48\n",
      "\n",
      "Matthews Correlation Coefficient (MCC): 0.8178368121442126\n"
     ]
    }
   ],
   "source": [
    "# Generate the classification report\n",
    "classification_report_ensemble = classification_report(\n",
    "    ensemble_df_cleaned['True_Label'], \n",
    "    ensemble_df_cleaned['Ensemble_Prediction']\n",
    ")\n",
    "print(\"Classification Report for Ensemble:\")\n",
    "print(classification_report_ensemble)\n",
    "\n",
    "# Calculate the Matthews Correlation Coefficient (MCC)\n",
    "mcc_ensemble = matthews_corrcoef(\n",
    "    ensemble_df_cleaned['True_Label'], \n",
    "    ensemble_df_cleaned['Ensemble_Prediction']\n",
    ")\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc_ensemble}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-certainty ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Certain Predictions: 41\n",
      "Number of Uncertain Predictions: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_l/gdx1m5nj6xq5zk07m__ljt280000gn/T/ipykernel_76346/836661484.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ensemble_df_cleaned['Certainty_Prediction'] = ensemble_df_cleaned.apply(calculate_certainty, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Add a new column for certainty-based predictions\n",
    "def calculate_certainty(row):\n",
    "    # Check if all predictions agree\n",
    "    if row['Pred_Label_Model1'] == row['Pred_Label_Model2'] == row['Pred_Label_Model3']:\n",
    "        return row['Pred_Label_Model1']  # Return the agreed prediction\n",
    "    else:\n",
    "        return -1  # Use -1 to indicate uncertainty\n",
    "\n",
    "# Apply the function to calculate certainty-based predictions\n",
    "ensemble_df_cleaned['Certainty_Prediction'] = ensemble_df_cleaned.apply(calculate_certainty, axis=1)\n",
    "\n",
    "# Count certain and uncertain predictions\n",
    "certain_count = (ensemble_df_cleaned['Certainty_Prediction'] != -1).sum()\n",
    "uncertain_count = (ensemble_df_cleaned['Certainty_Prediction'] == -1).sum()\n",
    "\n",
    "print(f\"Number of Certain Predictions: {certain_count}\")\n",
    "print(f\"Number of Uncertain Predictions: {uncertain_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Certain Predictions:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        29\n",
      "           1       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.98        41\n",
      "   macro avg       0.96      0.98      0.97        41\n",
      "weighted avg       0.98      0.98      0.98        41\n",
      "\n",
      "Matthews Correlation Coefficient (MCC) for Certain Predictions: 0.9440586233651661\n"
     ]
    }
   ],
   "source": [
    "# Filter for certain predictions\n",
    "certain_predictions = ensemble_df_cleaned[ensemble_df_cleaned['Certainty_Prediction'] != -1]\n",
    "\n",
    "# Generate classification report for certain predictions\n",
    "classification_report_certain = classification_report(\n",
    "    certain_predictions['True_Label'], \n",
    "    certain_predictions['Certainty_Prediction']\n",
    ")\n",
    "print(\"Classification Report for Certain Predictions:\")\n",
    "print(classification_report_certain)\n",
    "\n",
    "# Calculate MCC for certain predictions\n",
    "mcc_certain = matthews_corrcoef(\n",
    "    certain_predictions['True_Label'], \n",
    "    certain_predictions['Certainty_Prediction']\n",
    ")\n",
    "print(f\"Matthews Correlation Coefficient (MCC) for Certain Predictions: {mcc_certain}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further investigating of the first-visit model (against the high-certainty ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the first model performing on the cases, what the ensemble considers as high certainty cases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Model 1 on High-Certainty Predictions:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        29\n",
      "           1       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.98        41\n",
      "   macro avg       0.96      0.98      0.97        41\n",
      "weighted avg       0.98      0.98      0.98        41\n",
      "\n",
      "Matthews Correlation Coefficient (MCC) for Model 1 on High-Certainty Predictions: 0.9440586233651661\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the first model on high-certainty predictions\n",
    "classification_report_model1_high_certainty = classification_report(\n",
    "    certain_predictions['True_Label'], \n",
    "    certain_predictions['Pred_Label_Model1']\n",
    ")\n",
    "print(\"Classification Report for Model 1 on High-Certainty Predictions:\")\n",
    "print(classification_report_model1_high_certainty)\n",
    "\n",
    "# Calculate Matthews Correlation Coefficient (MCC) for Model 1 on high-certainty predictions\n",
    "mcc_model1_high_certainty = matthews_corrcoef(\n",
    "    certain_predictions['True_Label'], \n",
    "    certain_predictions['Pred_Label_Model1']\n",
    ")\n",
    "print(f\"Matthews Correlation Coefficient (MCC) for Model 1 on High-Certainty Predictions: {mcc_model1_high_certainty}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
